import Base: *
import LinearAlgebra: mul!, transpose!

```
BroadcastedOperators for basic operations
```
# x * y (aka matrix multiplication)
*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)
forward(::BroadcastedOperator{typeof(mul!)}, A, x) = A * x
backward(::BroadcastedOperator{typeof(mul!)}, A, x, âˆ‡) = let 
    # A: (outÃ—in), x: (inÃ—k), âˆ‡: (outÃ—m)
    # If x has 1 column but âˆ‡ has m>1 columns, treat x as "broadcast"
    if size(x, 2) == 1 && size(âˆ‡, 2) > 1
        # sum âˆ‡ over its columns first, then multiply by x'
        dA = sum(âˆ‡, dims=2) * x'      # (outÃ—1)*(1Ã—in) = (outÃ—in)
    else
        # normal batched or singleâ€col case
        dA = âˆ‡ * x'                  # (outÃ—batch)*(batchÃ—in) => (outÃ—in)
    end

    dX = A' * âˆ‡                      # always (inÃ—out)*(outÃ—batch) => (inÃ—batch)
    return (dA, dX)end

import LinearAlgebra: diagm
# x .* y (element-wise multiplication)
Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)
forward(::BroadcastedOperator{typeof(*)}, x, y) = x .* y
backward(node::BroadcastedOperator{typeof(*)}, x, y, âˆ‡) =
    # let
    #     ðŸ = ones(length(node.output))
    #     Jx = diagm(y .* ðŸ)
    #     Jy = diagm(x .* ðŸ)
    #     tuple(Jx' * âˆ‡, Jy' * âˆ‡)
    # end
    ( âˆ‡ .* y,  âˆ‡ .* x )

Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = 
    BroadcastedOperator(-, x, y)
forward(::BroadcastedOperator{typeof(-)}, x, y) = x.- y
backward(::BroadcastedOperator{typeof(-)}, x, y, âˆ‡) = tuple(âˆ‡, -âˆ‡)



Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = 
    BroadcastedOperator(+, x, y)
forward(::BroadcastedOperator{typeof(+)}, x, y) = x .+ y
backward(::BroadcastedOperator{typeof(+)}, x, y, âˆ‡) = tuple(âˆ‡, âˆ‡)



import Base: sum
sum(x::GraphNode) = BroadcastedOperator(sum, x)
forward(::BroadcastedOperator{typeof(sum)}, x) = sum(x)
backward(::BroadcastedOperator{typeof(sum)}, x, âˆ‡) = 
    # let
    #     ðŸ = ones(length(x))
    #     J = ðŸ'
    #     tuple(J' * âˆ‡)
    # end
    ( fill(âˆ‡, size(x)), )

import Statistics: mean
mean(x::GraphNode) = BroadcastedOperator(mean, x)
forward(::BroadcastedOperator{typeof(mean)}, x) =
    mean(x)
backward(::BroadcastedOperator{typeof(mean)}, x, âˆ‡) =
    let n = length(x)
        Î´ = fill(âˆ‡ / n, size(x))
    in
        tuple(Î´)
    end



# Potencjalnie do potegi ^-1 zamiast dzielenia w funkcji aktywacji.
Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) =
    BroadcastedOperator(/, x, y)
forward(::BroadcastedOperator{typeof(/)}, x, y) = x ./ y
backward(node::BroadcastedOperator{typeof(/)}, x, y, âˆ‡) = 
    let
        ðŸ = ones(length(node.output))
        Jx = diagm(ðŸ ./ y)
        Jy = (-x ./ y .^2)
        tuple(Jx' * âˆ‡, Jy' * âˆ‡)
    end
    # ( âˆ‡ ./ y,
    #  -âˆ‡ .* x ./ (y .^ 2) )


import Base: max
Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = 
    BroadcastedOperator(max, x, y)
forward(::BroadcastedOperator{typeof(max)}, x, y) = max.(x, y)
backward(::BroadcastedOperator{typeof(max)}, x, y, âˆ‡) = 
    let
        Jx = diagm(isless.(y, x))
        Jy = diagm(isless.(x, y))
        tuple(Jx' * âˆ‡, Jy' * âˆ‡)
    end
    # let mx = x .> y      # or `>=` to break ties differently
    #     in ( âˆ‡ .* mx, âˆ‡ .* .!mx )



Base.Broadcast.broadcasted(^, x::GraphNode, y::GraphNode) = 
    BroadcastedOperator(^, x, y)
forward(::BroadcastedOperator{typeof(^)}, x, y) = 
    x .^ y
backward(node::BroadcastedOperator{typeof(^)}, x, y, âˆ‡) = 
    let
        ðŸ = ones(length(node.output))
        Jx = diagm(y .* x .^ (y .- 1.0f0))
        Jy = diagm(log.(abs.(x)) .* x .^ y)
        tuple(Jx' * âˆ‡, Jy' * âˆ‡)
    end
    # ( âˆ‡ .* (y .* x .^ (y .- 1)),
    #   âˆ‡ .* (log.(abs.(x)) .* x .^ y) )

Base.Broadcast.broadcasted(exp, x::GraphNode) = 
    BroadcastedOperator(exp, x)
forward(::BroadcastedOperator{typeof(exp)}, x) = 
    exp.(x)
backward(node::BroadcastedOperator{typeof(exp)}, x, âˆ‡) = 
    let
        y = node.output
        J = diagm(y)
        tuple(J' * âˆ‡)
    end
    # ( âˆ‡ .* node.output, )

Base.Broadcast.broadcasted(log, x::GraphNode) = 
    BroadcastedOperator(log, x)
forward(::BroadcastedOperator{typeof(log)}, x) = 
    log.(x)
backward(::BroadcastedOperator{typeof(log)}, x, âˆ‡) = 
    # tuple(diagm(1.0 ./ x)' * âˆ‡)
    ( âˆ‡ ./ x, )


```
BroadcastedOperators for activation functions
```
Base.Broadcast.broadcasted(identity, x::GraphNode) = BroadcastedOperator(identity, x)
forward(::BroadcastedOperator{typeof(identity)}, x) = x
backward(::BroadcastedOperator{typeof(identity)}, x, âˆ‡) = 
    tuple(âˆ‡)

softmax(x::GraphNode) = BroadcastedOperator(softmax, x)
forward(::BroadcastedOperator{typeof(softmax)}, x) =
    let 
        max_x = maximum(x)
        exp_x_shifted = exp.(x .- max_x)
        sum_exp_x_shifted = sum(exp_x_shifted)
        y = exp_x_shifted ./ sum_exp_x_shifted 
    end
backward(node::BroadcastedOperator{typeof(softmax)}, x, âˆ‡) = 
    # let
    #     y = node.output
    #     J = diagm(y) .- y * y'
    #     tuple(J' * âˆ‡)
    # end
    let
        y = node.output
        Ï‰ = sum(âˆ‡ .* y)
        tuple(y .* (âˆ‡ .- Ï‰))
    end


sigmoid(x::GraphNode) = BroadcastedOperator(Ïƒ, x)
Ïƒ(x::GraphNode) = BroadcastedOperator(Ïƒ, x)
forward(::BroadcastedOperator{typeof(Ïƒ)}, x) = 1.0f0 ./ (1.0f0 .+ exp.(-x))
backward(node::BroadcastedOperator{typeof(Ïƒ)}, x, âˆ‡) = 
    # let
    #     y = node.output
    #     ðŸ = ones(length(y))
    #     J = diagm(y .* (1.0 .- y))
    #     tuple(J' * âˆ‡)
    # end
    let 
        y = node.output
        tuple(âˆ‡ .* (y .* (1.0f0 .- y)))
    end

import Base: tanh
tanh(x::GraphNode) = BroadcastedOperator(tanh, x)
forward(::BroadcastedOperator{typeof(tanh)}, x) = tanh.(x)
backward(node::BroadcastedOperator{typeof(tanh)}, x, âˆ‡) =
  let
    y = node.output
    tuple((1.0f0 .- y .^ 2.0f0) .* âˆ‡)
  end

ReLU(x::GraphNode) = BroadcastedOperator(ReLU, x)
forward(::BroadcastedOperator{typeof(ReLU)}, x) = max.(0.0f0, x)
backward(node::BroadcastedOperator{typeof(ReLU)}, x, âˆ‡) =
    let
        y    = node.output
        mask = y .> 0.0f0
        tuple(mask .* âˆ‡)            
    end

```
BroadcastedOperators for column extraction 
```
getindex_col(x::GraphNode, t::GraphNode) = BroadcastedOperator(getindex_col, x, t)
forward(::BroadcastedOperator{typeof(getindex_col)}, x::Array{Float32}, t::Int64) = @view x[:, t:t]
backward(::BroadcastedOperator{typeof(getindex_col)}, x::Matrix{Float32}, t::Int64, âˆ‡::Matrix{Float32}) = 
    let
        grad_x = zeros(eltype(âˆ‡), size(x))
        grad_x[:, t:t] .= âˆ‡
        (grad_x, nothing)
    end

getindex_col_batch(x::GraphNode, t::GraphNode) = BroadcastedOperator(getindex_col_batch, x, t)
forward(::BroadcastedOperator{typeof(getindex_col_batch)}, x::Array{Float32, 3}, t::Int64) =
    dropdims((@view x[:, t:t, :]), dims=2)
backward(::BroadcastedOperator{typeof(getindex_col_batch)},
         x::Array{Float32,3},    # <-- note the â€œ,3â€
         t::Int, 
         âˆ‡::Matrix{Float32}) =    # âˆ‡ has shape (embed_dim, batch)
    begin
        # grad_x must have the same shape as x did in forward: (embed_dim, seq_len, batch)
        grad_x = zeros(Float32, size(x))            # (embed_dim, seq_len, batch)
        # Put âˆ‡[:, b] into timeâ€step t for each batch index b:
        grad_x[:, t, :] .= âˆ‡                         # now (embed_dim, batch) fits into the 3rd dim
        return (grad_x, nothing)                     # âˆ‡ w.r.t. x is grad_x; no gradient for "t"
    end